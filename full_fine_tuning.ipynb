{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec009bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyrine.anene_amaris\\Documents\\whisper_fine_tuned\\Fine-tuning-Whisper-Air-Traffic-Control\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "import evaluate\n",
    "\n",
    "wer  = evaluate.load('wer')\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "def down_sample_audio(audio_original, original_sample_rate):\n",
    "    target_sample_rate = 16000\n",
    "\n",
    "    # Calculate the number of samples for the target sample rate\n",
    "    num_samples = int(len(audio_original) * target_sample_rate / original_sample_rate)\n",
    "\n",
    "    # Resample the audio array to the target sample rate\n",
    "    downsampled_audio = resample(audio_original, num_samples)\n",
    "\n",
    "    return downsampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60be65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\",language='english',task='transcribe')\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\",language='english',task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45da6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "atco_asr_data = load_dataset('parquet',data_files=\"dataset/train-00000-of-00005-c6681348ac8543dc.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf938f3",
   "metadata": {},
   "source": [
    "#### Maximum length of transription "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706f5db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_transcription_lengths = []\n",
    "\n",
    "for sample in atco_asr_data['train']:\n",
    "    text = sample['text']\n",
    "    tokenized_text = tokenizer(text).input_ids\n",
    "    list_of_transcription_lengths.append(len(tokenized_text))\n",
    "    # break\n",
    "\n",
    "plt.hist(list_of_transcription_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac95f4",
   "metadata": {},
   "source": [
    "Because of this hist, we chose 60 as our transcription size. If < 60, then it is padded (-100) and if > 60 it is truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89556469",
   "metadata": {},
   "source": [
    "#### Converting Dataset to Pytorch compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338fd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class whisper_training_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len):\n",
    "        self.dataset = dataset\n",
    "        self.max_len = max_len\n",
    "        self.bos_token = model.config.decoder_start_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        audio_data = down_sample_audio(item['audio'][\"array\"], item['audio'][\"sampling_rate\"])\n",
    "        input_features = feature_extractor(audio_data, sampling_rate=16000,return_tensors='pt').input_features[0]\n",
    "\n",
    "        # Process the transcription\n",
    "        transcription = item[\"text\"]\n",
    "\n",
    "        # Create labels\n",
    "        labels = tokenizer(transcription, padding=\"max_length\", max_length=self.max_len, truncation=True, return_tensors=\"pt\")\n",
    "        labels = labels[\"input_ids\"].masked_fill(labels['attention_mask'].ne(1), -100)\n",
    "        labels = labels[0][1:]\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fead137",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = whisper_training_dataset(dataset=atco_asr_data['train'], max_len=60)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Adjust batch size as needed\n",
    "    shuffle=True,  # Shuffle data during training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9397057",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42afeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model):\n",
    "\n",
    "    atco_asr_data = load_dataset('parquet',data_files=\"dataset/validation-00000-of-00002-7a5ea3756991bf72.parquet\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions=[]\n",
    "    references=[]\n",
    "\n",
    "    for sample in tqdm(atco_asr_data['train'],total=len(atco_asr_data['train'])):\n",
    "        audio=sample['audio']['array']\n",
    "        sample_rate=sample['audio']['sampling_rate']\n",
    "        text=sample['text']\n",
    "\n",
    "        audio = down_sample_audio(audio, sample_rate) # downsample the audio to 16000Hz for WHISPER\n",
    "\n",
    "        input_features = feature_extractor(\n",
    "        raw_speech=audio,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors='pt',\n",
    "        padding=True).input_features\n",
    "\n",
    "        # Generate predictions with no gradient computation\n",
    "        with torch.no_grad():\n",
    "            op = model.generate(input_features.to('cuda'), language='english', task='transcribe')\n",
    "\n",
    "        # Decode predictions\n",
    "        text_preds = tokenizer.batch_decode(op, skip_special_tokens=True)\n",
    "\n",
    "        # Append batch predictions and references to the respective lists\n",
    "        predictions.extend(text_preds)\n",
    "        references.extend([text])\n",
    "\n",
    "    WER = wer.compute(predictions=predictions, references=references) * 100\n",
    "\n",
    "    return WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc905d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Bacth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacth 2\n",
      "Bacth 3\n",
      "Bacth 4\n",
      "Bacth 5\n",
      "Bacth 6\n",
      "Bacth 7\n",
      "Bacth 8\n",
      "Bacth 9\n",
      "Bacth 10\n",
      "Bacth 11\n",
      "Bacth 12\n",
      "Bacth 13\n",
      "Bacth 14\n",
      "Bacth 15\n",
      "Bacth 16\n",
      "Bacth 17\n",
      "Bacth 18\n",
      "Bacth 19\n",
      "Bacth 20\n",
      "Bacth 21\n",
      "Bacth 22\n",
      "Bacth 23\n",
      "Bacth 24\n",
      "Bacth 25\n",
      "Bacth 26\n",
      "Bacth 27\n",
      "Bacth 28\n",
      "Bacth 29\n",
      "Bacth 30\n",
      "Bacth 31\n",
      "Bacth 32\n",
      "Bacth 33\n",
      "Bacth 34\n",
      "Bacth 35\n",
      "Bacth 36\n",
      "Bacth 37\n",
      "Bacth 38\n",
      "Bacth 39\n",
      "Bacth 40\n",
      "Bacth 41\n",
      "Bacth 42\n",
      "Bacth 43\n",
      "Bacth 44\n",
      "Bacth 45\n",
      "Bacth 46\n",
      "Bacth 47\n",
      "Bacth 48\n",
      "Bacth 49\n",
      "Bacth 50\n",
      "Bacth 51\n",
      "Bacth 52\n",
      "Bacth 53\n",
      "Bacth 54\n",
      "Bacth 55\n",
      "Bacth 56\n",
      "Bacth 57\n",
      "Bacth 58\n",
      "Bacth 59\n",
      "Bacth 60\n",
      "Bacth 61\n",
      "Bacth 62\n",
      "Bacth 63\n",
      "Bacth 64\n",
      "Bacth 65\n",
      "Bacth 66\n",
      "Bacth 67\n",
      "Bacth 68\n",
      "Bacth 69\n",
      "Bacth 70\n",
      "Bacth 71\n",
      "Bacth 72\n",
      "Bacth 73\n",
      "Bacth 74\n",
      "Bacth 75\n",
      "Bacth 76\n",
      "Bacth 77\n",
      "Bacth 78\n",
      "Bacth 79\n",
      "Bacth 80\n",
      "Bacth 81\n",
      "Bacth 82\n",
      "Bacth 83\n",
      "Bacth 84\n",
      "Bacth 85\n",
      "Bacth 86\n",
      "Bacth 87\n",
      "Bacth 88\n",
      "Bacth 89\n",
      "Bacth 90\n",
      "Bacth 91\n",
      "Bacth 92\n",
      "Bacth 93\n",
      "Bacth 94\n",
      "Bacth 95\n",
      "Bacth 96\n",
      "Bacth 97\n",
      "Bacth 98\n",
      "Bacth 99\n",
      "Bacth 100\n",
      "Bacth 101\n",
      "Bacth 102\n",
      "Bacth 103\n",
      "Bacth 104\n",
      "Bacth 105\n",
      "Bacth 106\n",
      "Bacth 107\n",
      "Bacth 108\n",
      "Bacth 109\n",
      "Bacth 110\n",
      "Bacth 111\n",
      "Bacth 112\n",
      "Bacth 113\n",
      "Bacth 114\n",
      "Bacth 115\n",
      "Bacth 116\n",
      "Bacth 117\n",
      "Bacth 118\n",
      "Bacth 119\n",
      "Bacth 120\n",
      "Bacth 121\n",
      "Bacth 122\n",
      "Bacth 123\n",
      "Bacth 124\n",
      "Bacth 125\n",
      "Bacth 126\n",
      "Bacth 127\n",
      "Bacth 128\n",
      "Bacth 129\n",
      "Bacth 130\n",
      "Bacth 131\n",
      "Bacth 132\n",
      "Bacth 133\n",
      "Bacth 134\n",
      "Bacth 135\n",
      "Bacth 136\n",
      "Bacth 137\n",
      "Bacth 138\n",
      "Bacth 139\n",
      "Bacth 140\n",
      "Bacth 141\n",
      "Bacth 142\n",
      "Bacth 143\n",
      "Bacth 144\n",
      "Bacth 145\n",
      "Bacth 146\n",
      "Bacth 147\n",
      "Bacth 148\n",
      "Bacth 149\n",
      "Bacth 150\n",
      "Bacth 151\n",
      "Bacth 152\n",
      "Bacth 153\n",
      "Bacth 154\n",
      "Bacth 155\n",
      "Bacth 156\n",
      "Bacth 157\n",
      "Bacth 158\n",
      "Bacth 159\n",
      "Bacth 160\n",
      "Bacth 161\n",
      "Bacth 162\n",
      "Bacth 163\n",
      "Bacth 164\n",
      "Bacth 165\n",
      "Bacth 166\n",
      "Bacth 167\n",
      "Bacth 168\n",
      "Bacth 169\n",
      "Bacth 170\n",
      "Bacth 171\n",
      "Bacth 172\n",
      "Bacth 173\n",
      "Bacth 174\n",
      "Bacth 175\n",
      "Bacth 176\n",
      "Bacth 177\n",
      "Bacth 178\n",
      "Bacth 179\n",
      "Bacth 180\n",
      "Bacth 181\n",
      "Bacth 182\n",
      "Bacth 183\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "max_epochs=1\n",
    "\n",
    "running_wer=[]\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    i =0\n",
    "    for batch in train_dataloader:\n",
    "        i= i+ 1\n",
    "        print(f\"Bacth {i}\")\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        input_features, labels = batch[\"input_features\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_features, labels=labels)  # Assuming your model takes these inputs\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    running_wer.append(evaluation(model))\n",
    "    plt.plot(running_wer)\n",
    "    clear_output(wait=True)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('wer (%)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('finetuned_atco.pth', weights_only=True))# loading the model\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ec003",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "atco_asr_data = load_dataset('parquet',data_files=\"validation-00000-of-00002-7a5ea3756991bf72.parquet\")\n",
    "\n",
    "for idx in range(5):\n",
    "\n",
    "    target = atco_asr_data['train'][idx]['text']\n",
    "    audio_original = atco_asr_data['train'][idx]['audio']['array']\n",
    "\n",
    "    input_feature = feature_extractor(raw_speech=audio_original,\n",
    "                                    sampling_rate=16000,\n",
    "                                    return_tensors='pt').input_features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_feature.to('cuda'), language='english', task='transcribe')\n",
    "\n",
    "\n",
    "    text_pred =  tokenizer.batch_decode(op,skip_special_tokens=True )[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'true : {target} \\npred : {text_pred}')\n",
    "    print('\\n ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
